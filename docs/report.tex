\title{Parallelized Sorting Algorithms}
\author{Ben Dewan \& Youlian Simidjiyski}
\documentclass{article}
\begin{document}
\maketitle

\section{Concept}
Analyzing the performance of sorting algorithms is a fairly common task, especially in introductory computer science courses. At this point the performance characteristics of CPU-based, sequential sorting algorithms are well documented and studied. However, with the advent of GPGPU programming, we felt that, since many aspects of sorting are intuitively parallel, it would be interesting to examine the performance gains of implementing several well known sorting algorithms as GPGPU applications. We used NVIDIA's CUDA to implement four different algorithms (heap sort, merge sort, quick sort and radix sort) to measure possible performance gains over CPU-based implementations of the same algorithms, as well as learn the capabilities and limits of the GPGPU platform as it exists today.

\section{Implementation}
Our main goal was to compare the performance of the same sorting algorithm, implemented sequentially on the CPU and parallelized on the GPU. To utilize the GPU, our code was written using 'C for CUDA,' developed by NVIDIA, as well as framework code and helper functions written with C. We now explain the specifics of each implementation.
\begin{enumerate}
  \setlength{\itemsep}{10pt}
  \setlength{\parskip}{10pt}

  \item Heap Sort

  \item Merge Sort 

  \item Quick Sort\\ 
        Our CPU-based implementation of Quicksort is based upon the 'Quicksort with three-way partitioning' by Robert Sedgewick[1]. This particular implementation uses a a more aggressive partitioning scheme, splitting the list into three parts based upon the pivot value (less than, equal and greater than the pivot value), which can improve performance in situations involving duplicate key values, but otherwise follows a standard implementation of quicksort.\\
        Our GPU-based implementation of quicksort is not particularly sophisticated. With NVIDIA GPUs that have Compute Capability 2.0 or lower, kernel functions (functions executed on the GPU) cannot be called recursively. Further complications arise when attempting to sort lists that are larger than will fit within a single thread block, because there can be no data dependencies between thread blocks, and partitioning and iterating on the GPU would require determining new pivots for the sublists, that is sharing data between thread blocks. We therefore compromised, exploiting the parallel nature of comparing values against the pivot and partitioning the list on the GPU, but making recursive function calls on the Host.\\
  \item Radix Sort\\
        Our CPU-based implementation of Radix Sort is an implementation of 'Radix Quicksort,' again by Robert Sedgewick[1]. The algorithm works by performing bitwise comparisons and partitioning the list based on the value of a particular bit (0 or 1), and then recursively calling itself on the sublists. As with quicksort, once the sublists become short enough, the overhead of performing all the comparisons and partitioning becomes a hindrance, so once the sublists being sorted decrease to lengths less than 10 elements, the list is sorted using insertion sort.\\
        The GPU-based implementation of this algorithm is very similar to that of Quicksort, however there is no pivot value, which streamlines the comparison and partitioning process (which in turn decreases the complexity of the kernel). Given a list to be sorted, we use the host to recursively call our bitwise comparison and partition kernel function.
\end{enumerate}
For more indepth information, please view the source code, available at\\ 'http://github.com/ysimidjiyski/CS222SearchSort/'

\section{Testing and Results}
\section{Conclusions}
\section{Next Steps}
There are two major steps to take from this point. The first being further optimization, the second being analyzing sorting algorithms designed specifically for GPGPU processing.\\
Optimization, never a particularly easy process, is made more difficult due to the amount of control given to the developer over memory allocation and the lack of sophisticated programming models. Only with NVIDIA devices with Compute Capability 2.0 or higher (Fermi devices or later) support recursion, and the small stack and high cost of executing conditional and branch code on devices makes improving performance on a complicated kernel difficult. Also, whereas when one writes code to a general purpose processor, memory management is usually controlled by the operating system, hardware and/or the compiler, CUDA relies entirely on the developer to efficiently manage the caches to maximize performance. Whether this is due to the relative infancy of the platform (specifically the compiler) or an explicit design decision, it makes optimizing for CUDA much more involved, as cache optimizations must be explicit and there are no safeguards in place to prevent overflow (which can crash ones graphics drivers very easily).\\
Another question we should consider is, even after optimizing these algorithms, we should compare the results against some of the various algorithms designed specifically as GPGPU applications. The relatively parallel nature of sorting has led to the development of algorithms such as Bitonic-Merge Sort and Even-Odd Sorting Networks specifically for GPUs, and it would be interesting to see if they are quicker than a properly adapted and optimized verion of Quicksort (which is, on average, the quickest sequential sorting algorithm). It may be that the idea of 'parallelizing' algorithms may be particularly inefficient, and therefore is worth testing.
\section{References}
$[1]$ Robert Sedgewick, {\em Algorithms in C}, Third Edition.\\
$[2]$ Daniel Cederman $\&$ Philippas Tsigas, "A Practical Quicksort ALgorithm for Graphics Processors," 2008. http://www.cse.chalmers.se/research/group/dcs/TechReports/gpusort.pdf\\
$[3]$ Nadathur Satish, Mark Harris $\&$ Michael Garland, "Designing Efficient Sorting Algorithms for Manycore GPUs," 2009. http://mgarland.org/files/papers/gpu-sort-idps09.pdf\\
\end{document}
